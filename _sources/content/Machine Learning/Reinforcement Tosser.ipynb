{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Reinforcement Tosser\n",
    "- Subtitle: Beating an Interview Question, now with RL\n",
    "- Date: 2018-10-15 15:05\n",
    "- Category: Puzzle\n",
    "- Tags: reinforcement-learning, puzzle, gym, interview\n",
    "- Author: Varun Nayyar\n",
    "- Status: draft\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "See [part 1](/blog/tosser). This is still in draft.\n",
    "\n",
    "This is not going to be a treatise on reinforcement learning. For that, you should check out [this excellent blog](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html), but I think trying to decide how to bet given my current multiplier and number of tosses left in order to optimize sounds like something a simple reinforcment learning approach could use.\n",
    "\n",
    "I'm going to use [OpenAI's gym](https://gym.openai.com/) to build this as it's a nice standard model of doing things that pre-existing code can handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the cell/env you copy if you have your own agent to test\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete\n",
    "import random\n",
    "from math import ceil\n",
    "\n",
    "class Tosser(Env):\n",
    "    def __init__(self, numToss=8, initialAmount=100, nA=101):\n",
    "        \"\"\"\n",
    "        Each action is to be a percentage point of remaining amount, rounded up\n",
    "        State is numToss, currAmnt\n",
    "        \n",
    "        Is done when numToss or currAmnt are 0.\n",
    "        \"\"\"\n",
    "        self.numToss = self.tossLeft = numToss\n",
    "        self.initialAmount = self.currAmnt = initialAmount\n",
    "        self.action_space = Discrete(nA)  # 0..100 inclusive!\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        random.seed(seed)\n",
    "\n",
    "    def reset(self):\n",
    "        self.tossLeft = self.numToss\n",
    "        self.currAmnt = self.initialAmount\n",
    "        return (self.tossLeft, self.currAmnt)\n",
    "    \n",
    "    def amntBet(self, action):\n",
    "        lam = action/(self.action_space.n-1)\n",
    "        return ceil(lam * self.currAmnt)\n",
    "    \n",
    "    def done(self):\n",
    "        return (self.currAmnt == 0) or (self.tossLeft == 0)\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        assert not self.done()\n",
    "        \n",
    "        amntBet = self.amntBet(action)\n",
    "        \n",
    "        if random.random() > 0.5:  # heads\n",
    "            self.currAmnt += 3 * amntBet\n",
    "            info = \"Win\"\n",
    "        else:\n",
    "            self.currAmnt -= amntBet\n",
    "            info = \"Loss\"\n",
    "\n",
    "        self.tossLeft -= 1\n",
    "        \n",
    "        reward = self.currAmnt/self.initialAmount if self.done() else 0\n",
    "        return (self.tossLeft, self.currAmnt), reward, self.done(), info            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure to optimize.\n",
    "\n",
    "The standard reinforcement technique, Qtables, that I'm using sort of tracks the mean using this equation\n",
    "$Q(s,a) = (1-\\alpha) Q(s,a) + \\alpha * [reward + \\gamma * max (Q(s_{new}))]$. As we can see, this is effectively a weighted sum of the past samples, where more recent samples have a heavier weight. However, since we have a skewed distribution with a lot of samples biased to larger values, this will skew the measure too and we'll optimize for a measure similar to the average as opposed to the median result\n",
    "\n",
    "As a result, we store the result of every trial in the Q table and return the median. We use the same discount equation, but we update each state visited once after every trial. This is a slow process since the median can't be calculated in a streaming method, but we optimize by using a sortedlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## support code - this is not well documented and pulled from some prior work in this space\n",
    "## Will move to a library eventually\n",
    "import numpy as np\n",
    "from sortedcontainers import SortedList\n",
    "\n",
    "def medianFromSorted(slist):\n",
    "    if len(slist) % 2 == 1:\n",
    "        return slist[len(slist)//2]\n",
    "    else:\n",
    "        i1 = len(slist)//2\n",
    "        i2 = i1-1\n",
    "        return (slist[i1] + slist[i2])/2\n",
    "        \n",
    "\n",
    "class QDynamicTable:\n",
    "    \n",
    "    def __init__(self, nA=4, nS=None):\n",
    "        from collections import defaultdict\n",
    "        self.num_actions = nA\n",
    "        self.Q = defaultdict(lambda: [SortedList() for i in range(nA)])\n",
    "\n",
    "    def get_Q(self, s, a):\n",
    "        \"\"\"Q(s, a): get the Q value of (s, a) pair\"\"\"\n",
    "        if self.Q[s][a]:\n",
    "            return medianFromSorted(self.Q[s][a])\n",
    "        return 0\n",
    "\n",
    "    def get_max(self, s):\n",
    "        \"\"\"max Q(s): get the max of all Q value of state s\"\"\"\n",
    "        return max(self.get_Q(s, a) for a in range(self.num_actions))\n",
    "\n",
    "    def set_Q(self, s, a, q):\n",
    "        \"\"\"Q(s, a) = q: update the q value of (s, a) pair\"\"\"\n",
    "        self.Q[s][a].add(q)\n",
    "\n",
    "    def get_max_a(self, s):\n",
    "        \"\"\"argmax_a Q(s, a): get the action which has the highest Q in state s\"\"\"\n",
    "        mx = self.get_max(s)\n",
    "        for a in range(self.num_actions):\n",
    "            if self.get_Q(s, a) == mx:\n",
    "                return a\n",
    "        return random.randint(0, self.num_actions)\n",
    "            \n",
    "    def __str__(self):\n",
    "        output = []\n",
    "        for s in self.Q:\n",
    "            output.append(s.__str__() + \": \" + [\"{:07.4f}\".format(self.get_Q(s, a) or 0) for a in range(self.num_actions)].__str__())\n",
    "        output.sort()\n",
    "        return \"QTable (number of actions = \" + str(self.num_actions) + \", states = \" + str(\n",
    "            len(output)) + \"):\\n\" + \"\\n\".join(output)\n",
    "\n",
    "\n",
    "    \n",
    "class Epsilon:\n",
    "    \"\"\"\n",
    "    Gratuitous class for the epsilon greedy part of reinforcement learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, start=1.0, end=0.01, update_decrement=0.01):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.update_decrement = update_decrement\n",
    "        self._value = self.start\n",
    "        self.isTraining = True\n",
    "\n",
    "    def decrement(self, count=1):\n",
    "        self._value = max(self.end, self._value - self.update_decrement * count)\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.isTraining:\n",
    "            return self._value\n",
    "        else:\n",
    "            # always explore\n",
    "            return 0\n",
    "\n",
    "    @value.setter\n",
    "    def value(self, val):\n",
    "        self._value = val\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "To keep things simple we discretize/quantize the states of the game's current winnings. This is done via some trial and error and realisation that the game will skew left with a long tail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agent code here\n",
    "\n",
    "This uses qtable learning, basically we convert number of tosses left and amount earned\n",
    "into a set of finite states. As we see more and more run throughs, we get a sense of which\n",
    "actions result in a higher multipler\n",
    "\n",
    "This is somewhat akin to dynamic programming, but since in this case we don't have perfect\n",
    "substructure in that the location of our multipler changes depending on how previous tosses/bets\n",
    "went and once we've developed a high multiplier, we don't want to keep betting in a risky sense\n",
    "\"\"\"\n",
    "\n",
    "alpha = 0.4 # don't learn too quickly \n",
    "gamma = 0.99 # value future actions high-ish\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, numToss=8, initialAmount=100, nA=11):\n",
    "        self.numToss = numToss\n",
    "        self.initialAmount = initialAmount\n",
    "        self.nA = nA\n",
    "        \n",
    "        self.env = Tosser(numToss, initialAmount, nA)\n",
    "        self.epsilon = Epsilon(start=1.0, end=0.05, update_decrement=0.002)\n",
    "        \n",
    "        self.moneybins = self.getMoneyBins()\n",
    "        self.Q = QDynamicTable(nA=nA)\n",
    "\n",
    "        \n",
    "    def getMoneyBins(self):\n",
    "        # i expect more bins in the early game\n",
    "        bn = (np.linspace(0, 20, 100, endpoint=False),\n",
    "              np.linspace(20, 100, 40, endpoint=False),\n",
    "              np.linspace(100, 250, 30, endpoint=False),\n",
    "              np.linspace(250, max(2 ** self.numToss, 251), 20))\n",
    "        return np.concatenate(bn)\n",
    "\n",
    "    def getAction(self, s):\n",
    "        if self.epsilon.value > random.random():\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return self.Q.get_max_a(s)\n",
    "\n",
    "    def discretize(self, obs):\n",
    "        \"\"\"\n",
    "        take observations and return a state\n",
    "        \"\"\"\n",
    "        return obs[0], np.digitize(obs[1]/self.initialAmount, self.moneybins).item()\n",
    "\n",
    "    def train(self, episodes=100, debug=False):\n",
    "        self.epsilon.isTraining = True\n",
    "        maxreward = 0\n",
    "        for i in range(episodes):\n",
    "            if i % (episodes / 10) == 0:\n",
    "                print(f\"Episode: {i} of {episodes}, eps: {self.epsilon.value}\")\n",
    "            done = False\n",
    "            s = self.discretize(self.env.reset())\n",
    "            action_list = []\n",
    "            if debug: print(f\"Epsiode: {i}, start={s}, eps={self.epsilon.value}\")\n",
    "            while not done:\n",
    "                a = self.getAction(s)\n",
    "                action_list.append((s, a))\n",
    "                s_1, reward, done, info = self.env.step(a)\n",
    "                if done:\n",
    "                    # don't crowd up the q matrix\n",
    "                    s_1 = (0,0)\n",
    "\n",
    "#                 if debug: print(s_1)\n",
    "                s_1 = self.discretize(s_1)\n",
    "                # this is the usual equation, however, I need to track the median\n",
    "                # so an approach like the below will only approximate the mean\n",
    "                # and using this approach also results in a lot of untrue 0's if i track the median\n",
    "                # instead I store the state in a reverse approach\n",
    "#                 newq = alpha*(reward + gamma * self.Q.get_max(s_1)) + (1-alpha) * self.Q.get_Q(s,a)\n",
    "                if debug: print(f\"action: {a/(self.nA-1) * 100:.2f}, info: {info}\" \\\n",
    "                                f\"Tosses left: {self.env.tossLeft}, currAmnt={self.env.currAmnt}, state={s_1}\" \\\n",
    "                                f\", reward={reward}, newq={newq:.2f}, eps={self.epsilon.value}\")\n",
    "#                 self.Q.set_Q(s, a, newq)\n",
    "                s = s_1\n",
    "            gammcnt = gamma\n",
    "            # here the action_state list and visited in a reverse method, updating\n",
    "            # the states we've seen so far while discounting gamma\n",
    "            for state, act in reversed(action_list):\n",
    "#                 print(f\"updating s:{state}, act: {act}, r: {reward * gammcnt}\")\n",
    "                self.Q.set_Q(state, act, gammcnt * reward)\n",
    "                gammcnt *= gamma            \n",
    "        \n",
    "            self.epsilon.decrement(0.005)\n",
    "\n",
    "    def run(self):\n",
    "        self.epsilon.isTraining = False\n",
    "        s = self.discretize(self.env.reset())\n",
    "        steps = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.getAction(s)\n",
    "            s_1_f, reward, done, info = self.env.step(action)\n",
    "            s_1 = self.discretize(s_1_f)\n",
    "\n",
    "#             print(f\"Done: {done}, Curr State: {s}, Action {['L', 'R'][action]}, New State: {s_1_f}\")\n",
    "            s = s_1\n",
    "\n",
    "        self.env.close()\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 of 100000, eps: 1.0\n",
      "Episode: 10000 of 100000, eps: 0.9000000000004551\n",
      "Episode: 20000 of 100000, eps: 0.8000000000009102\n",
      "Episode: 30000 of 100000, eps: 0.7000000000013653\n",
      "Episode: 40000 of 100000, eps: 0.6000000000018204\n",
      "Episode: 50000 of 100000, eps: 0.5000000000022755\n",
      "Episode: 60000 of 100000, eps: 0.4000000000021755\n",
      "Episode: 70000 of 100000, eps: 0.3000000000020755\n",
      "Episode: 80000 of 100000, eps: 0.2000000000019755\n",
      "Episode: 90000 of 100000, eps: 0.10000000000191017\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(8)\n",
    "agent.train(episodes=100000, debug=False)\n",
    "#print(agent.Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.55, 2.07, 7.96]), 7.974539999999999)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gamma = 0.99\n",
    "nruns = 1000\n",
    "ressy = np.zeros(nruns)\n",
    "for i in range(1000):\n",
    "    ressy[i] = agent.run()\n",
    "    \n",
    "np.percentile(ressy, [25,50,75]), np.mean(ressy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.48, 3.33, 4.9 ]), 9.62875)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gamma = 0.9\n",
    "nruns = 1000\n",
    "ressy = np.zeros(nruns)\n",
    "for i in range(1000):\n",
    "    ressy[i] = agent.run()\n",
    "    \n",
    "np.percentile(ressy, [25,50,75]), np.mean(ressy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.73, 2.2 , 6.24]), 6.388739999999999)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gamma = 0.3\n",
    "nruns = 1000\n",
    "ressy = np.zeros(nruns)\n",
    "for i in range(1000):\n",
    "    ressy[i] = agent.run()\n",
    "    \n",
    "np.percentile(ressy, [25,50,75]), np.mean(ressy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This is a very rudimentary application of reinforcement learning (and I hope to revisit this question with DQN and other deep learning frameworks), that I have shown above. \n",
    "\n",
    "The higher gamma values optimize for the upper tails, while lower optimizes for the lower tails. We can see training with gamma=0.9 gives a median result of 3.33 > best possible play with a constant $\\lambda$ which is nice! We could run this again with the Qtable's measure returning 25% and 75% to see what values we would get."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
