Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.10.6/x64/lib/python3.10/site-packages/jupyter_cache/executors/utils.py", line 51, in single_nb_execution
    executenb(
  File "/opt/hostedtoolcache/Python/3.10.6/x64/lib/python3.10/site-packages/nbclient/client.py", line 1204, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/opt/hostedtoolcache/Python/3.10.6/x64/lib/python3.10/site-packages/nbclient/util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
  File "/opt/hostedtoolcache/Python/3.10.6/x64/lib/python3.10/site-packages/nbclient/util.py", line 62, in just_run
    return loop.run_until_complete(coro)
  File "/opt/hostedtoolcache/Python/3.10.6/x64/lib/python3.10/asyncio/base_events.py", line 646, in run_until_complete
    return future.result()
  File "/opt/hostedtoolcache/Python/3.10.6/x64/lib/python3.10/site-packages/nbclient/client.py", line 663, in async_execute
    await self.async_execute_cell(
  File "/opt/hostedtoolcache/Python/3.10.6/x64/lib/python3.10/site-packages/nbclient/client.py", line 965, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/opt/hostedtoolcache/Python/3.10.6/x64/lib/python3.10/site-packages/nbclient/client.py", line 862, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
# set gamma to be quite high since we want to stay upright as long as possible - this has quite
# a large effect on the reward. 
# a lot of the growth is likely to be due to the epsilon value, not just the training
# using a faster epsilon growth makes this very obvious
# a more successful agent is slower to train due to the number of successes it obtains
# the longer it goes for. In this case, the env is limited to 200 timesteps
ag = Agent(QTable, gamma=0.99)
ns, eps = ag.train(25000)

# record, 
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
Cell [0;32mIn [5], line 8[0m
[1;32m      1[0m [38;5;66;03m# set gamma to be quite high since we want to stay upright as long as possible - this has quite[39;00m
[1;32m      2[0m [38;5;66;03m# a large effect on the reward. [39;00m
[1;32m      3[0m [38;5;66;03m# a lot of the growth is likely to be due to the epsilon value, not just the training[39;00m
[1;32m      4[0m [38;5;66;03m# using a faster epsilon growth makes this very obvious[39;00m
[1;32m      5[0m [38;5;66;03m# a more successful agent is slower to train due to the number of successes it obtains[39;00m
[1;32m      6[0m [38;5;66;03m# the longer it goes for. In this case, the env is limited to 200 timesteps[39;00m
[1;32m      7[0m ag [38;5;241m=[39m Agent(QTable, gamma[38;5;241m=[39m[38;5;241m0.99[39m)
[0;32m----> 8[0m ns, eps [38;5;241m=[39m [43mag[49m[38;5;241;43m.[39;49m[43mtrain[49m[43m([49m[38;5;241;43m25000[39;49m[43m)[49m

Cell [0;32mIn [2], line 51[0m, in [0;36mAgent.train[0;34m(self, num_epsiodes, initeps, finaleps)[0m
[1;32m     49[0m [38;5;28;01mwhile[39;00m [38;5;129;01mnot[39;00m done:
[1;32m     50[0m     action [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mget_action(state, epsilon[38;5;241m=[39mepsilon)
[0;32m---> 51[0m     new_state, reward, done, info [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39menv[38;5;241m.[39mstep(action)
[1;32m     52[0m     [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m done:
[1;32m     53[0m         [38;5;66;03m# add the future reward * decay if we're still going[39;00m
[1;32m     54[0m         reward [38;5;241m+[39m[38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mgamma [38;5;241m*[39m [38;5;28mself[39m[38;5;241m.[39mQstate[38;5;241m.[39mget_max(new_state)

[0;31mValueError[0m: too many values to unpack (expected 4)
ValueError: too many values to unpack (expected 4)

